<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>UniRestorer</title>
    <link href="./files/style.css" rel="stylesheet">
    <script type="text/javascript" src="./files/jquery.mlens-1.0.min.js"></script>
    <script type="text/javascript" src="./files/jquery.js"></script>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
<div class="content">
    <h2 align="center"><strong>UniRestorer: Universal Image Restoration via <br> Adaptively Estimating Image Degradation at Proper Granularity</strong></h2>
  <p id="authors"><a href="https://scholar.google.com/citations?user=UpOaYLoAAAAJ&hl=zh-CN">Jingbo Lin<sup>1</sup></a><a
        href="https://scholar.google.com/citations?user=8pIq2N0AAAAJ&hl=zh-CN">Zhilu Zhang<sup>1</sup></a><a
        href="https://github.com/YngMgC">Wenbo Li<sup>2</sup></a><a
        href="https://scholar.google.com/citations?user=4LsZhDgAAAAJ&hl=zh-CN&oi=ao">Renjing Pei<sup>2</sup></a>
      <br>
      <a
        href="https://scholar.google.com/citations?user=I4vuL7kAAAAJ&hl=zh-CN&oi=ao">Hang Xu<sup>2</sup></a><a
        href="https://scholar.google.com/citations?user=I4vuL7kAAAAJ&hl=zh-CN&oi=ao">Hongzhi Zhang<sup>1</sup></a><a
        href="https://scholar.google.com/citations?user=rUOpCEYAAAAJ&hl=zh-CN">Wangmeng Zuo<sup>1</sup></a><br>
    <br>
    <!--        <span style="font-size: 24px">KAIST-->
    <span style="font-size: 20px;"><sup>1</sup>Harbin Institute of Technology</span>               <span style="font-size: 20px;"><sup>2</sup>Huawei Noah's Ark Lab</span>

    <p style="text-align: center;font-size: 20px;">
        <a href="https://arxiv.org/abs/2412.20157" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/mrluin/UniRestorer" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
    </p>

    </span></p>

    <br>
    <img src="./files/radar.png" class="teaser-gif" style="width:100%;"><br>
    <p style="text-align: center;">
        Comparisons with task-agnostic methods, all-in-one methods, and task-specific models on all-in-one image restoration.
        <b> (a) </b> Comparisons on single-degradation setting.
        <b> (b) </b> Comparisons with specific single-task models.
        <b> (c) </b> Comparisons on mixed-degradation setting <i> (in-distribution) </i>.
        <b> (d) </b> Comparisons on mixed-degradation setting <i> (out-of-distribution) </i>.
    </p>
</div>

<div class="content">
    <h2>Abstract</h2>
    <p>
    Recently, considerable progress has been made in all-in-one image restoration.
    Generally, existing methods can be degradation-agnostic or degradation-aware.
    However, the former are limited in leveraging degradation-specific restoration, and the latter suffer from the inevitable error in degradation estimation.
    Consequently, the performance of existing methods has a large gap compared to specific single-task models.
    In this work, we make a step forward in this topic, and present our UniRestorer with improved restoration performance.
    Specifically, we perform hierarchical clustering on degradation space, and train a multi-granularity mixture-of-experts (MoE) restoration model.
    Then, UniRestorer adopts both degradation and granularity estimation to adaptively select an appropriate expert for image restoration.
    In contrast to existing degradation-agnostic and -aware methods, UniRestorer can leverage degradation estimation to benefit degradation-specific restoration, and use granularity estimation to make the model robust to degradation estimation error.
    Experimental results show that our UniRestorer outperforms state-of-the-art all-in-one methods by a large margin, and is promising in closing the performance gap to specific single-task models.
    The code and pre-trained models will be publicly available.
    </p>
</div>

<div class="content">
    <h2>Method</h2>
    <img class="summary-img" src="./files/framework.png" style="width:100%;"> <br>
    <p>  <b>(a) Training of Deblur4DGS.</b>
        When processing \(t\)-th frame, we first discretize its exposure time into \(N\) timestamps.
        Then,  we estimate continuous camera poses \(\{\mathbf{P}_{t,i}\}_{i=1}^{N}\) and dynamic Gaussians \(\{\mathbf{D}_{t,i}\}_{i=1}^{N}\) within exposure time.
        Next, we render each latent sharp image \(\hat{\mathbf{I}}_{t,i}\) with the camera pose \(\mathbf{P}_{t,i}\), dynamic Gaussians \(\mathbf{D}_{t,i}\) and static Gaussians \(\mathbf{S}\).
        Finally, \(\{\hat{\mathbf{I}}_{t,i}\}_{i=1}^{N}\) are averaged to obtain the synthetic blurry image \(\hat{\mathbf{B}}_{t}\), which is used to calculate the reconstruction loss \(\mathcal{L}_{rec}\) with the given blurry frame \(\mathbf{B}_{t}\).
        To regularize the under-constrained optimization, we introduce  exposure regularization \(\mathcal{L}_{e}\), multi-frame consistency regularization \(\mathcal{L}_{mfc}\) and multi-resolution consistency regularization \(\mathcal{L}_{mrc}\).
        <b>(b) Rendering of Deblur4DGS.</b> Deblur4DGS produces the sharp image with user-provided timestamp \(t\) and camera pose \(\mathbf{P}_{t}\).</p>
    <br>

</div>

</body>

</html>
